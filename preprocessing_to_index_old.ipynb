{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435b504d-8310-4d23-8efb-f80491811e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-2.8.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from openai) (4.11.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Downloading jiter-0.12.0-cp313-cp313-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.41.5-cp313-cp313-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-2.8.1-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 10.3 MB/s  0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.12.0-cp313-cp313-win_amd64.whl (204 kB)\n",
      "Downloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 22.5 MB/s  0:00:00\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, pydantic-core, jiter, distro, annotated-types, pydantic, openai\n",
      "\n",
      "   ----- ---------------------------------- 1/7 [pydantic-core]\n",
      "   ---------------------------- ----------- 5/7 [pydantic]\n",
      "   ---------------------------- ----------- 5/7 [pydantic]\n",
      "   ---------------------------- ----------- 5/7 [pydantic]\n",
      "   ---------------------------- ----------- 5/7 [pydantic]\n",
      "   ---------------------------- ----------- 5/7 [pydantic]\n",
      "   ---------------------------- ----------- 5/7 [pydantic]\n",
      "   ---------------------------- ----------- 5/7 [pydantic]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------------- 7/7 [openai]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 jiter-0.12.0 openai-2.8.1 pydantic-2.12.4 pydantic-core-2.41.5 typing-inspection-0.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e21b83f6-0088-4ee1-964c-5c91aeb0ba45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-ai-documentintelligence\n",
      "  Downloading azure_ai_documentintelligence-1.0.2-py3-none-any.whl.metadata (53 kB)\n",
      "Requirement already satisfied: isodate>=0.6.1 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from azure-ai-documentintelligence) (0.7.2)\n",
      "Requirement already satisfied: azure-core>=1.30.0 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from azure-ai-documentintelligence) (1.36.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from azure-ai-documentintelligence) (4.15.0)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from azure-core>=1.30.0->azure-ai-documentintelligence) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-documentintelligence) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-documentintelligence) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-documentintelligence) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-documentintelligence) (2025.10.5)\n",
      "Downloading azure_ai_documentintelligence-1.0.2-py3-none-any.whl (106 kB)\n",
      "Installing collected packages: azure-ai-documentintelligence\n",
      "Successfully installed azure-ai-documentintelligence-1.0.2\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (4.14.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install azure-ai-documentintelligence\n",
    "! pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c6791a-b680-484d-8635-254c1b6bbb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get document intelligence resource\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "doc_intel_endpoint = os.getenv(\"DOC_INTEL_ENDPOINT\")\n",
    "doc_intel_key = os.getenv(\"DOC_INTEL_KEY\")\n",
    "doc_intel_client = DocumentIntelligenceClient(endpoint=doc_intel_endpoint, credential=AzureKeyCredential(doc_intel_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59dcc466-e1dc-4175-94e0-7d2f4c9faaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(InvalidRequest) Invalid request.\n",
      "Code: InvalidRequest\n",
      "Message: Invalid request.\n",
      "Exception Details:\t(UnsupportedContent) Content is not supported: The input content is corrupted or format is invalid.\n",
      "\tCode: UnsupportedContent\n",
      "\tMessage: Content is not supported: The input content is corrupted or format is invalid.\n",
      "\tTarget: 0\n",
      "(InvalidRequest) Invalid request.\n",
      "Code: InvalidRequest\n",
      "Message: Invalid request.\n",
      "Exception Details:\t(UnsupportedContent) Content is not supported: The input content is corrupted or format is invalid.\n",
      "\tCode: UnsupportedContent\n",
      "\tMessage: Content is not supported: The input content is corrupted or format is invalid.\n",
      "\tTarget: 0\n",
      "(InvalidRequest) Invalid request.\n",
      "Code: InvalidRequest\n",
      "Message: Invalid request.\n",
      "Exception Details:\t(UnsupportedContent) Content is not supported: The input content is corrupted or format is invalid.\n",
      "\tCode: UnsupportedContent\n",
      "\tMessage: Content is not supported: The input content is corrupted or format is invalid.\n",
      "\tTarget: 0\n",
      "(InvalidRequest) Invalid request.\n",
      "Code: InvalidRequest\n",
      "Message: Invalid request.\n",
      "Exception Details:\t(UnsupportedContent) Content is not supported: The input content is corrupted or format is invalid.\n",
      "\tCode: UnsupportedContent\n",
      "\tMessage: Content is not supported: The input content is corrupted or format is invalid.\n",
      "\tTarget: 0\n",
      "(InvalidRequest) Invalid request.\n",
      "Code: InvalidRequest\n",
      "Message: Invalid request.\n",
      "Exception Details:\t(UnsupportedContent) Content is not supported: The input content is corrupted or format is invalid.\n",
      "\tCode: UnsupportedContent\n",
      "\tMessage: Content is not supported: The input content is corrupted or format is invalid.\n",
      "\tTarget: 0\n",
      "(InvalidRequest) Invalid request.\n",
      "Code: InvalidRequest\n",
      "Message: Invalid request.\n",
      "Exception Details:\t(UnsupportedContent) Content is not supported: The input content is corrupted or format is invalid.\n",
      "\tCode: UnsupportedContent\n",
      "\tMessage: Content is not supported: The input content is corrupted or format is invalid.\n",
      "\tTarget: 0\n",
      "(InvalidRequest) Invalid request.\n",
      "Code: InvalidRequest\n",
      "Message: Invalid request.\n",
      "Exception Details:\t(UnsupportedContent) Content is not supported: The input content is corrupted or format is invalid.\n",
      "\tCode: UnsupportedContent\n",
      "\tMessage: Content is not supported: The input content is corrupted or format is invalid.\n",
      "\tTarget: 0\n",
      "(InvalidRequest) Invalid request.\n",
      "Code: InvalidRequest\n",
      "Message: Invalid request.\n",
      "Exception Details:\t(UnsupportedContent) Content is not supported: The input content is corrupted or format is invalid.\n",
      "\tCode: UnsupportedContent\n",
      "\tMessage: Content is not supported: The input content is corrupted or format is invalid.\n",
      "\tTarget: 0\n",
      "(InvalidRequest) Invalid request.\n",
      "Code: InvalidRequest\n",
      "Message: Invalid request.\n",
      "Exception Details:\t(UnsupportedContent) Content is not supported: The input content is corrupted or format is invalid.\n",
      "\tCode: UnsupportedContent\n",
      "\tMessage: Content is not supported: The input content is corrupted or format is invalid.\n",
      "\tTarget: 0\n",
      "(InvalidRequest) Invalid request.\n",
      "Code: InvalidRequest\n",
      "Message: Invalid request.\n",
      "Exception Details:\t(UnsupportedContent) Content is not supported: The input content is corrupted or format is invalid.\n",
      "\tCode: UnsupportedContent\n",
      "\tMessage: Content is not supported: The input content is corrupted or format is invalid.\n",
      "\tTarget: 0\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "#Generate markdowns\n",
    "input_dir =\"./project-assurance-data\"\n",
    "output_dir = './markdown_files'\n",
    "os.makedirs(output_dir,exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    with open(input_dir+\"/\"+filename, \"rb\") as file:\n",
    "        html_content = file.read()\n",
    "    try:\n",
    "        # Convert to markdown with Document Intelligence\n",
    "        poller = doc_intel_client.begin_analyze_document(\"prebuilt-layout\",\n",
    "                                                        body=html_content,\n",
    "                                                        content_type=\"application/octet-stream\")\n",
    "        result = poller.result()\n",
    "        markdown = result['content']\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        # If Document Intelligence fails to convert HTML to markdown, use Beautiful soap\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        markdown = soup.get_text()\n",
    "        \n",
    "    new_filename = filename[:-4]+\"txt\"\n",
    "    with open(output_dir+\"/\"+new_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(markdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7aa77fa9-273f-41fd-b9da-f4bd99e1dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade --quiet langchain-text-splitters tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01e72389-5096-454b-a7b1-e0a2290a6863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "#Chunking\n",
    "chunk_dir = \"./chunks\"\n",
    "MAX_CHUNK_TOKEN_SIZE = 512 # Max number of tokens for chunking\n",
    "CHUNK_OVERLAPPING=0.25 # 25% of overlapping between chunks\n",
    "\n",
    "os.makedirs(chunk_dir, exist_ok=True)\n",
    "for filename in os.listdir(output_dir):\n",
    "    with open(output_dir+\"/\"+filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    text_splitter = TokenTextSplitter(chunk_size=MAX_CHUNK_TOKEN_SIZE, chunk_overlap=int(MAX_CHUNK_TOKEN_SIZE*CHUNK_OVERLAPPING))\n",
    "    \n",
    "    chunks = text_splitter.split_text(text)\n",
    "    for j,chunk in enumerate(chunks):\n",
    "        new_filename = filename[:-3]+\"_\"+str(j)+\".txt\"\n",
    "        with open(chunk_dir+\"/\"+new_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f41d60e3-3000-4854-8e43-a6fbabd49b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-search-documents\n",
      "  Downloading azure_search_documents-11.6.0-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: azure-core>=1.28.0 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from azure-search-documents) (1.36.0)\n",
      "Requirement already satisfied: azure-common>=1.1 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from azure-search-documents) (1.1.28)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from azure-search-documents) (0.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from azure-search-documents) (4.15.0)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from azure-core>=1.28.0->azure-search-documents) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\benja\\anaconda3\\envs\\testml\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (2025.10.5)\n",
      "Downloading azure_search_documents-11.6.0-py3-none-any.whl (307 kB)\n",
      "Installing collected packages: azure-search-documents\n",
      "Successfully installed azure-search-documents-11.6.0\n"
     ]
    }
   ],
   "source": [
    "! pip install azure-search-documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b14fca-33b0-4725-9807-e9fc915b0d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField, SearchFieldDataType, SearchableField, SearchField, VectorSearch, HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile, SemanticConfiguration, SemanticPrioritizedFields, SemanticField, SemanticSearch,\n",
    "    SearchIndex, VectorSearchAlgorithmKind, HnswParameters, VectorSearchAlgorithmMetric\n",
    ")\n",
    "##CREATE INDEX##\n",
    "#Create search endpoint\n",
    "ai_search_endpoint = os.getenv(\"AI_SEARCH_ENDPOINT\")\n",
    "ai_search_apikey = os.getenv(\"AI_SEARCH_KEY\")\n",
    "ai_search_index_name = os.getenv(\"AI_SEARCH_INDEX_NAME\")\n",
    "ai_search_credential = AzureKeyCredential(ai_search_apikey)\n",
    "index_client = SearchIndexClient(endpoint=ai_search_endpoint, credential=ai_search_credential)\n",
    "\n",
    "dimensions = 1536\n",
    "# Fields definition\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"embeddingTitle\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=dimensions, vector_search_profile_name=\"myHnswProfile\"),\n",
    "    SearchField(name=\"embeddingContent\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=dimensions, vector_search_profile_name=\"myHnswProfile\")\n",
    "]\n",
    "\n",
    "# Configure the vector search configuration\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\",\n",
    "            kind=VectorSearchAlgorithmKind.HNSW,\n",
    "            parameters=HnswParameters(\n",
    "                m=4,\n",
    "                ef_construction=400,\n",
    "                ef_search=500,\n",
    "                metric=VectorSearchAlgorithmMetric.COSINE\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHnsw\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Semantic ranker configuration\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        content_fields=[SemanticField(field_name=\"content\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=ai_search_index_name, fields=fields, vector_search=vector_search, semantic_search=semantic_search)\n",
    "result = index_client.create_or_update_index(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3e1264-e19b-4983-a5f7-9818ccf62d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 72\n",
      "INDEXING BATCH 32\n",
      "Waiting 15 seconds...\n",
      "INDEXING BATCH 64\n",
      "Waiting 15 seconds...\n",
      "INDEXING BATCH 96\n",
      "Waiting 15 seconds...\n",
      "INDEXING BATCH 128\n",
      "Waiting 15 seconds...\n",
      "INDEXING BATCH 160\n",
      "Waiting 15 seconds...\n",
      "Waiting 15 seconds...\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchIndexingBufferedSender\n",
    "from openai import AzureOpenAI\n",
    "import time\n",
    "##FILL INDEX WITH CHUNKS##\n",
    "#Create embedding endpoint\n",
    "endpoint = os.getenv(\"FOUNDRY_ENDPOINT\")\n",
    "subscription_key = os.getenv(\"FOUNDRY_KEY\")\n",
    "embedding_model_name = os.getenv(\"EMBEDDING_MODEL_NAME\")\n",
    "api_version = \"2024-12-01-preview\"\n",
    "embedding_client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key\n",
    ")\n",
    "\n",
    "\n",
    "batch_client = SearchIndexingBufferedSender(\n",
    "                endpoint=ai_search_endpoint,\n",
    "                index_name=ai_search_index_name,\n",
    "                credential=ai_search_credential\n",
    "            )\n",
    "chunk_files = os.listdir(chunk_dir)\n",
    "lote = []\n",
    "for i, chunk_name in  enumerate(chunk_files):\n",
    "    with open(chunk_dir+\"/\"+chunk_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    title = chunk_name[:-4]\n",
    "    document = {\n",
    "        \"id\": str(i),\n",
    "        \"title\": title,\n",
    "        \"content\": content,\n",
    "        \"embeddingTitle\": embedding_client.embeddings.create(input=title, model=embedding_model_name).data[0].embedding,\n",
    "        \"embeddingContent\": embedding_client.embeddings.create(input=content, model=embedding_model_name).data[0].embedding,\n",
    "    }\n",
    "    lote.append(document)\n",
    "    if (i + 1) % 32 == 0:\n",
    "        # Upload documents\n",
    "        print(f'INDEXING BATCH {i + 1}')\n",
    "        try:\n",
    "            batch_client.upload_documents(documents=lote)\n",
    "            print('Waiting 15 seconds...')\n",
    "            time.sleep(15)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "        lote = []\n",
    "if len(lote)>0:\n",
    "    try:\n",
    "        batch_client.upload_documents(documents=lote)\n",
    "        print('Waiting 15 seconds...')\n",
    "        time.sleep(15)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df139789-ef6c-428b-8ef2-ea3d2912a470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
